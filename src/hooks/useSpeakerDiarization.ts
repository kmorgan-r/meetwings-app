/**
 * useSpeakerDiarization - Hook for batch speaker diarization (Phase 3)
 *
 * This hook manages speaker diarization for meeting transcripts:
 * - Receives diarization results from AssemblyAI
 * - Matches results to existing transcript entries
 * - Assigns consistent speaker labels across the session
 * - Handles cross-batch speaker tracking
 */

import { useCallback, useRef } from "react";
import {
  fetchAssemblyAIWithDiarization,
  AssemblyAIUtterance,
} from "@/lib/functions/assemblyai.function";
import {
  DiarizationAudioBuffer,
  BufferedSegment,
  concatenateAudioBlobs,
} from "@/lib/functions/audio-buffer";
import {
  analyzePitch,
  PitchAnalysisResult,
} from "@/lib/functions/pitch-analysis";
import {
  findProfileByPitch,
  createAutoProfile,
  updateProfilePitch,
} from "@/lib/storage/speaker-profiles.storage";
import type { TranscriptEntry, SpeakerInfo } from "@/types";

/**
 * Per-batch speaker mapping (AssemblyAI labels A, B, C reset per batch).
 * Enhanced with pitch-based profile matching.
 */
interface BatchSpeakerMap {
  [diarizationLabel: string]: {
    sessionId: string; // "speaker_1", "speaker_2", or profile ID
    displayName: string; // "Speaker 1", profile name, or "Speaker N (Unnamed)"
    color: string;
    profileId?: string; // Link to SpeakerProfile if matched
    pitchData?: PitchAnalysisResult; // Voice characteristics for this speaker
  };
}

/**
 * Session-level speaker registry for consistent labeling.
 */
interface SessionSpeakerRegistry {
  /** Counter for assigning new speaker numbers */
  nextSpeakerNumber: number;
  /** Per-batch mappings (AssemblyAI labels reset per batch) */
  batchMappings: { [batchId: string]: BatchSpeakerMap };
  /** Global speaker assignments (persists across batches) */
  assignedSpeakers: {
    [sessionId: string]: {
      displayName: string;
      color: string;
    };
  };
}

/**
 * Speaker color palette for visual distinction.
 */
const SPEAKER_COLORS = [
  "#22c55e", // green
  "#f97316", // orange
  "#8b5cf6", // purple
  "#ec4899", // pink
  "#14b8a6", // teal
  "#eab308", // yellow
];

interface UseSpeakerDiarizationProps {
  /** AssemblyAI API key */
  apiKey: string;
  /** Language code for transcription */
  language?: string;
  /** Expected number of speakers (optional, improves accuracy) */
  speakersExpected?: number;
  /** Callback to update a specific transcript entry's speaker info */
  updateEntrySpeaker: (timestamp: number, speakerInfo: SpeakerInfo) => void;
  /** Get current transcript entries for matching */
  getTranscriptEntries: () => TranscriptEntry[];
  /** Batch duration in milliseconds (default 30000) */
  batchDurationMs?: number;
  /** Callback for errors */
  onError?: (error: Error) => void;
  /** Callback when diarization completes for a batch */
  onBatchProcessed?: (batchId: string, speakerCount: number) => void;
}

export function useSpeakerDiarization({
  apiKey,
  language,
  speakersExpected,
  updateEntrySpeaker,
  getTranscriptEntries,
  batchDurationMs = 30000,
  onError,
  onBatchProcessed,
}: UseSpeakerDiarizationProps) {
  // Session-level speaker registry
  const speakerRegistryRef = useRef<SessionSpeakerRegistry>({
    nextSpeakerNumber: 1,
    batchMappings: {},
    assignedSpeakers: {},
  });

  // Track processing state
  const isProcessingRef = useRef(false);

  /**
   * Process a batch of audio segments through AssemblyAI diarization.
   */
  const processBatch = useCallback(
    async (segments: BufferedSegment[], batchId: string) => {
      if (segments.length === 0 || !apiKey) return;
      if (isProcessingRef.current) {
        console.warn(
          "[SpeakerDiarization] Already processing, skipping batch"
        );
        return;
      }

      isProcessingRef.current = true;

      try {
        console.log(
          `[SpeakerDiarization] Processing batch ${batchId} with ${segments.length} segments`
        );

        // Get batch start time from first segment
        const batchStartTime = segments[0].timestamp;

        // Concatenate audio blobs
        const audioBlobs = segments.map((s) => s.audio);
        const concatenatedAudio = await concatenateAudioBlobs(audioBlobs);

        console.log(
          `[SpeakerDiarization] Concatenated audio size: ${concatenatedAudio.size} bytes`
        );

        // Send to AssemblyAI with diarization
        const result = await fetchAssemblyAIWithDiarization(concatenatedAudio, {
          apiKey,
          language,
          speakersExpected,
        });

        console.log(
          `[SpeakerDiarization] Received ${result.utterances.length} utterances`
        );

        if (result.utterances.length === 0) {
          console.log("[SpeakerDiarization] No utterances found in batch");
          return;
        }

        // Get current transcript entries for matching
        const transcriptEntries = getTranscriptEntries();

        // Match diarization results to transcript entries (with pitch-based recognition)
        const uniqueSpeakers = new Set<string>();
        await matchDiarizationResults(
          transcriptEntries,
          result.utterances,
          batchStartTime,
          batchId,
          speakerRegistryRef.current,
          updateEntrySpeaker,
          uniqueSpeakers,
          segments // Pass original segments for pitch analysis
        );

        console.log(
          `[SpeakerDiarization] Matched ${uniqueSpeakers.size} unique speakers in batch`
        );

        onBatchProcessed?.(batchId, uniqueSpeakers.size);
      } catch (error) {
        // Security: Sanitize error before logging (never expose API keys)
        const errorMessage = error instanceof Error ? error.message : String(error);
        console.error("[SpeakerDiarization] Batch processing failed:", errorMessage);
        onError?.(
          error instanceof Error ? error : new Error("Diarization failed")
        );
      } finally {
        isProcessingRef.current = false;
      }
    },
    [
      apiKey,
      language,
      speakersExpected,
      updateEntrySpeaker,
      getTranscriptEntries,
      onError,
      onBatchProcessed,
    ]
  );

  /**
   * Create a DiarizationAudioBuffer instance that will call processBatch when ready.
   */
  const createAudioBuffer = useCallback((): DiarizationAudioBuffer => {
    return new DiarizationAudioBuffer(batchDurationMs, processBatch);
  }, [batchDurationMs, processBatch]);

  /**
   * Reset the speaker registry (e.g., when starting a new meeting).
   */
  const resetRegistry = useCallback(() => {
    speakerRegistryRef.current = {
      nextSpeakerNumber: 1,
      batchMappings: {},
      assignedSpeakers: {},
    };
  }, []);

  /**
   * Get the current speaker registry (for debugging/display).
   */
  const getRegistry = useCallback(() => {
    return speakerRegistryRef.current;
  }, []);

  return {
    /** Create a new AudioBuffer for buffering segments */
    createAudioBuffer,
    /** Process a batch manually (for testing or force-processing) */
    processBatch,
    /** Reset the speaker registry for a new meeting */
    resetRegistry,
    /** Get the current speaker registry */
    getRegistry,
    /** Whether diarization is currently processing */
    isProcessing: isProcessingRef.current,
  };
}

/**
 * Match diarization results to transcript entries with pitch-based speaker recognition.
 *
 * This function:
 * 1. Analyzes pitch for each unique speaker in the batch
 * 2. Matches pitch against existing speaker profiles
 * 3. Creates auto-profiles for unmatched speakers
 * 4. Updates transcript entries with matched/created profiles
 * 5. Updates profiles with new pitch data (learning over time)
 */
async function matchDiarizationResults(
  transcriptEntries: TranscriptEntry[],
  utterances: AssemblyAIUtterance[],
  batchStartTime: number,
  batchId: string,
  speakerRegistry: SessionSpeakerRegistry,
  updateEntry: (timestamp: number, speaker: SpeakerInfo) => void,
  uniqueSpeakers: Set<string>,
  segments: BufferedSegment[]
): Promise<void> {
  // Create new batch mapping (AssemblyAI labels "A", "B" reset per batch)
  if (!speakerRegistry.batchMappings[batchId]) {
    speakerRegistry.batchMappings[batchId] = {};
  }
  const batchMap = speakerRegistry.batchMappings[batchId];

  // Phase 1: Analyze pitch for each unique speaker
  // Group utterances by speaker to find representative samples
  const speakerUtterances = new Map<string, AssemblyAIUtterance[]>();
  for (const utterance of utterances) {
    if (!speakerUtterances.has(utterance.speaker)) {
      speakerUtterances.set(utterance.speaker, []);
    }
    speakerUtterances.get(utterance.speaker)!.push(utterance);
  }

  // For each speaker, analyze pitch from their audio segments
  for (const [diarizationLabel, speakerUtts] of speakerUtterances.entries()) {
    // Find audio segment(s) for this speaker
    // Use the longest utterance as it's likely to have the best pitch data
    const longestUtterance = speakerUtts.reduce((longest, utt) =>
      (utt.end - utt.start) > (longest.end - longest.start) ? utt : longest
    );

    const uttAbsTime = batchStartTime + longestUtterance.start;

    // Find the segment that best matches this utterance's timing
    const matchingSegment = findClosestSegment(segments, uttAbsTime);

    if (matchingSegment) {
      try {
        // Analyze pitch from this speaker's audio (with performance monitoring)
        const startTime = performance.now();
        const pitchData = await analyzePitch(matchingSegment.audio);
        const duration = performance.now() - startTime;

        console.log(
          `[SpeakerDiarization] Analyzed pitch for ${diarizationLabel}: ${pitchData.avgHz.toFixed(0)} Hz (${duration.toFixed(0)}ms)`
        );

        // Warn if pitch analysis is slow (may impact UX on older devices)
        if (duration > 50) {
          console.warn(
            `[Performance] Pitch analysis took ${duration.toFixed(0)}ms - consider device optimization`
          );
        }

        // Try to match against existing profiles
        const matchedProfile = await findProfileByPitch(pitchData, 80);

        if (matchedProfile) {
          // Matched existing profile - use it
          console.log(
            `[SpeakerDiarization] Matched ${diarizationLabel} to existing profile: ${matchedProfile.name}`
          );

          batchMap[diarizationLabel] = {
            sessionId: matchedProfile.id,
            displayName: matchedProfile.name,
            color: matchedProfile.color,
            profileId: matchedProfile.id,
            pitchData,
          };

          // Update profile with new pitch data (learning)
          await updateProfilePitch(matchedProfile.id, pitchData);
        } else {
          // No match - create auto-profile
          const sampleText = longestUtterance.text.slice(0, 100);
          const newProfile = await createAutoProfile(pitchData, batchId, sampleText);

          console.log(
            `[SpeakerDiarization] Created auto-profile for ${diarizationLabel}: ${newProfile.name}`
          );

          batchMap[diarizationLabel] = {
            sessionId: newProfile.id,
            displayName: newProfile.name,
            color: newProfile.color,
            profileId: newProfile.id,
            pitchData,
          };
        }

        // Register in global assigned speakers
        speakerRegistry.assignedSpeakers[batchMap[diarizationLabel].sessionId] = {
          displayName: batchMap[diarizationLabel].displayName,
          color: batchMap[diarizationLabel].color,
        };
      } catch (error) {
        // Security: Sanitize error before logging
        const errorMessage = error instanceof Error ? error.message : String(error);
        console.error(
          `[SpeakerDiarization] Pitch analysis failed for ${diarizationLabel}, using fallback:`,
          errorMessage
        );
        // Fallback: Use sequential numbering if pitch analysis fails
        createFallbackSpeaker(diarizationLabel, batchMap, speakerRegistry);
      }
    } else {
      console.warn(
        `[SpeakerDiarization] No matching segment for ${diarizationLabel}, using fallback`
      );
      // Fallback: Use sequential numbering if no segment found
      createFallbackSpeaker(diarizationLabel, batchMap, speakerRegistry);
    }
  }

  // Phase 2: Update all transcript entries with matched profiles
  for (const utterance of utterances) {
    // Calculate absolute timestamp from batch-relative time
    const absoluteTimestamp = batchStartTime + utterance.start;

    // Find matching transcript entry (with performance monitoring)
    const matchStartTime = performance.now();
    const entry = findBestMatchingEntry(
      transcriptEntries,
      utterance,
      absoluteTimestamp
    );
    const matchDuration = performance.now() - matchStartTime;

    // Warn if entry matching is slow (indicates too many candidates in long meetings)
    if (matchDuration > 100) {
      // Only calculate candidate count when logging (avoid overhead in fast path)
      const candidateCount = transcriptEntries.filter(
        (e) => e.audioSource === "system" && !e.speaker?.confirmed
      ).length;
      console.warn(
        `[Performance] Entry matching took ${matchDuration.toFixed(0)}ms with ${candidateCount} candidates ` +
        `(transcript size: ${transcriptEntries.length}). Consider optimizing for long meetings.`
      );
    }

    if (entry) {
      const speakerMapping = batchMap[utterance.speaker];
      if (speakerMapping) {
        uniqueSpeakers.add(speakerMapping.sessionId);

        // Update entry with speaker info
        updateEntry(entry.timestamp, {
          speakerId: speakerMapping.sessionId,
          speakerLabel: speakerMapping.displayName,
          confirmed: speakerMapping.profileId ? true : false, // Confirmed if matched profile
          confidence: utterance.confidence,
        });
      }
    }
  }
}

/**
 * Create fallback speaker with sequential numbering (used when pitch analysis fails).
 */
function createFallbackSpeaker(
  diarizationLabel: string,
  batchMap: BatchSpeakerMap,
  speakerRegistry: SessionSpeakerRegistry
): void {
  const speakerNum = speakerRegistry.nextSpeakerNumber++;
  const sessionId = `speaker_${speakerNum}`;
  batchMap[diarizationLabel] = {
    sessionId,
    displayName: `Speaker ${speakerNum}`,
    color: SPEAKER_COLORS[(speakerNum - 1) % SPEAKER_COLORS.length],
  };
  speakerRegistry.assignedSpeakers[sessionId] = {
    displayName: batchMap[diarizationLabel].displayName,
    color: batchMap[diarizationLabel].color,
  };
}

/**
 * Find the audio segment closest to a given timestamp.
 */
function findClosestSegment(
  segments: BufferedSegment[],
  targetTimestamp: number
): BufferedSegment | undefined {
  if (segments.length === 0) return undefined;

  return segments.reduce((closest, segment) => {
    const closestDiff = Math.abs(closest.timestamp - targetTimestamp);
    const segmentDiff = Math.abs(segment.timestamp - targetTimestamp);
    return segmentDiff < closestDiff ? segment : closest;
  });
}

/**
 * Find the best matching transcript entry for a diarization utterance.
 * Uses both timestamp proximity and text similarity for matching.
 */
function findBestMatchingEntry(
  transcriptEntries: TranscriptEntry[],
  utterance: AssemblyAIUtterance,
  absoluteTimestamp: number
): TranscriptEntry | undefined {
  // Filter to candidates (system audio, not yet confirmed)
  // Performance optimization: Limit to most recent 100 candidates to prevent
  // unbounded growth in long meetings (2+ hours). Since we're matching recent
  // utterances from the batch, the most recent transcript entries are most relevant.
  // This prevents O(n) growth where n = meeting duration.
  const candidates = transcriptEntries
    .filter((e) => {
      // Must be system audio and not yet confirmed
      if (e.audioSource !== "system") return false;
      if (e.speaker?.confirmed) return false;
      return true;
    })
    .slice(-100); // Limit to last 100 unconfirmed entries

  if (candidates.length === 0) return undefined;

  // Time window for matching (1.5 seconds)
  const TIME_WINDOW_MS = 1500;

  // Score each candidate
  const scored = candidates.map((entry) => {
    const timeDiff = Math.abs(entry.timestamp - absoluteTimestamp);

    // Early exit optimization: Skip expensive text similarity calculation
    // if time difference is too large (>4.5 seconds).
    // This preserves the ability to match on text alone (see line 483) while
    // avoiding wasted computation on clearly wrong candidates.
    // In long meetings (100+ entries), this reduces Levenshtein calls by 70-90%.
    const MAX_TIME_TOLERANCE = TIME_WINDOW_MS * 3; // 4.5 seconds
    if (timeDiff > MAX_TIME_TOLERANCE) {
      return { entry, score: 0, timeDiff, textSimilarity: 0 };
    }

    const textSimilarity = calculateTextSimilarity(
      entry.original,
      utterance.text
    );

    // Calculate combined score (prefer time proximity + text match)
    let score = 0;

    // Time score: 1.0 at exact match, 0 at boundary
    if (timeDiff <= TIME_WINDOW_MS) {
      score += (1 - timeDiff / TIME_WINDOW_MS) * 0.5;
    }

    // Text similarity score (weighted more heavily)
    score += textSimilarity * 0.5;

    return { entry, score, timeDiff, textSimilarity };
  });

  // Sort by score descending
  scored.sort((a, b) => b.score - a.score);

  // Return best match if score is above threshold
  const best = scored[0];
  // Require either good time match OR good text match (not necessarily both)
  if (best && (best.timeDiff <= TIME_WINDOW_MS || best.textSimilarity > 0.7)) {
    return best.entry;
  }

  return undefined;
}

/**
 * Calculate text similarity using Levenshtein distance.
 */
function calculateTextSimilarity(a: string, b: string): number {
  const longer = a.length > b.length ? a : b;
  const shorter = a.length > b.length ? b : a;

  if (longer.length === 0) return 1.0;

  const editDistance = levenshteinDistance(
    longer.toLowerCase(),
    shorter.toLowerCase()
  );
  return (longer.length - editDistance) / longer.length;
}

/**
 * Calculate Levenshtein edit distance between two strings.
 */
function levenshteinDistance(a: string, b: string): number {
  const matrix: number[][] = [];

  for (let i = 0; i <= b.length; i++) {
    matrix[i] = [i];
  }
  for (let j = 0; j <= a.length; j++) {
    matrix[0][j] = j;
  }

  for (let i = 1; i <= b.length; i++) {
    for (let j = 1; j <= a.length; j++) {
      if (b.charAt(i - 1) === a.charAt(j - 1)) {
        matrix[i][j] = matrix[i - 1][j - 1];
      } else {
        matrix[i][j] = Math.min(
          matrix[i - 1][j - 1] + 1,
          matrix[i][j - 1] + 1,
          matrix[i - 1][j] + 1
        );
      }
    }
  }

  return matrix[b.length][a.length];
}
